from dotenv import load_dotenv
from hana_ml import dataframe
import os
import json

load_dotenv()

host = str(os.getenv("HANA_HOST"))


def get_hana_db():
    connection = dataframe.ConnectionContext(
        address=host,
        port=443,
        user=str(os.getenv("HANA_USER")),
        password=str(os.getenv("HANA_PASSWORD")),
    )
    return connection


# getting connection object
conn = get_hana_db()


create_table_sql = """
CREATE COLUMN TABLE documents_embedding (
    id INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    document_text NVARCHAR(5000),
    embedding REAL_VECTOR(1536),
    chunk_metadata NVARCHAR(1000),
    ref_id NVARCHAR(36) UNIQUE NOT NULL
)
"""

# Executing table creation
conn.execute_sql(create_table_sql)
print("table created successfully")


def ensure_triple_store():
    """
    create TRIPLE STORE table and indexes if they don't exist
    """
    conn = get_hana_db()
    try:
        exists_sql = """
        SELECT 1 FROM SYS.TABLES
        WHERE TABLE_NAME = 'TRIPLE_STORE'
        AND SCHEMA_NAME = CURRENT_SCHEMA
        """
        if conn.sql(exists_sql).collect():
            return  # already exits

        create_sql = """
        CREATE COLUMN TABLE TRIPLE_STORE (
        ID BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
        EMB_REF_ID NVARCHAR(36) UNIQUE NOT NULL,
        CHUNK_INDEX INTEGER,
        SUBJECT NVARCHAR(500),
        PREDICATE NVARCHAR(200),
        OBJECT NVARCHAR(1000),
        CREATED_AT TIMESTAMP DEFAULT CURRENT_UTCTIMESTAMP,
        FOREIGN KEY (EMB_REF_ID) REFERENCES documents_embedding(ref_id)
        )
        """
        conn.execute_sql(create_sql)
        conn.execute_sql("CREATE INDEX IDX_TRIPLE_SUBJ ON TRIPLE_STORE (SUBJECT)")
        conn.execute_sql("CREATE INDEX IDX_TRIPLE_PRED ON TRIPLE_STORE (PREDICATE)")
        conn.execute_sql("CREATE INDEX IDX_TRIPLE_OBJ ON TRIPLE_STORE (OBJECT)")
        conn.execute_sql(
            "CREATE INDEX IDX_TRIPLE_DOC ON TRIPLE_STORE (ref_id, CHUNK_INDEX)"
        )
        print("triple store and indexes")

    except Exception as e:
        print(f"error ensuring triple store: {e}")


def insert_triplets(all_triplets: list, EMB_REF_ID=None, chunk_index=None):
    """
    all_triplets: list of (subject, predicate, object)
    EMB_REF_ID, chunk_index: optional provenance
    """
    ensure_triple_store()
    conn = get_hana_db()
    rows = [
        (EMB_REF_ID, chunk_index, subj, pred, obj)
        for (subj, pred, obj) in all_triplets
    ]
    sql = """
        INSERT INTO TRIPLE_STORE (EMB_REF_ID, CHUNK_INDEX, SUBJECT, PREDICATE, OBJECT)
        VALUES (?, ?, ?, ?, ?)
            """
    try:
        cur = conn.connection.cursor()
        cur.executemany(sql, rows)
        conn.connection.commit()
        cur.close()
        print(f"inserted {len(rows)} triplets")
        return True
    except Exception as e:
        print(f"error inserting triplets: {e}")
        return False

# this function will insert embeddings and will return the document ID's
def batch_insertion_embedding(rows):
    conn_ctx = get_hana_db()
    """
    args:
        - rows : list of tuples -> [(document_text: str, embedding_string: str, metadata_json: str, ref_id: str), ...]
    """
    sql = "INSERT INTO documents_embedding (document_text, embedding, chunk_metadata, ref_id) VALUES (?, TO_REAL_VECTOR(?), ?, ?)"
    
    try:
        cursor = conn_ctx.connection.cursor()
        cursor.executemany(sql, rows)
        conn_ctx.connection.commit()
        cursor.close()
        return True
    except Exception as e:
        print(f"failed to execute batch insert: {e}")
        return False


def insert_embedding(document_text, embedding_vector, chunk_metadata=None):
    conn = get_hana_db()

    # escaping text for SQL
    escaped_text = document_text.replace("'", "''")

    # Converting metadata dict to JSON string and escaping it
    if chunk_metadata:
        metadata_json = json.dumps(chunk_metadata)
        escaped_metadata = metadata_json.replace("'", "''")
    else:
        escaped_metadata = "{}"

    insert_sql = f"""
    INSERT INTO documents_embedding (document_text, embedding, chunk_metadata)
    VALUES ('{escaped_text}', TO_REAL_VECTOR('{str(embedding_vector)}'), '{escaped_metadata}')
    """

    try:
        conn.execute_sql(insert_sql)
        print("successfully inserted document and embedding")
        return True
    except Exception as e:
        return False


# function to find top k similiar documents
# ...existing code...
def search_similiar_documents(query_embedding, top_k=5):
    conn = get_hana_db()

    # Escape and format inputs
    vec = str(query_embedding).replace("'", "''")
    k = int(top_k) if top_k else 5
    if k <= 0:
        k = 5

    search_sql = f"""
    SELECT DOCUMENT_TEXT,
           CHUNK_METADATA,
           COSINE_SIMILARITY(EMBEDDING, TO_REAL_VECTOR('{vec}')) AS SIMILARITY
    FROM DOCUMENTS_EMBEDDING
    ORDER BY SIMILARITY DESC
    LIMIT {k}
    """
    try:
        df = conn.sql(search_sql)
        results = df.collect()
        print(results)
        return results
    except Exception as e:
        print(f"error searching similar documents: {e}")
        return None


# function to get all the data from database
def get_all_data():
    conn = get_hana_db()
    sql = "SELECT * FROM documents_embedding"
    try:
        df = conn.sql(sql)
        results = df.collect()
        print(results)
        return results
    except Exception as e:
        print(f"error fetching all data: {e}")
        return None

from dotenv import load_dotenv
from hana_ml import dataframe
import os
import json
import logging

load_dotenv()
logger = logging.getLogger(__name__)

host = str(os.getenv("HANA_HOST"))


def get_hana_db():
    connection = dataframe.ConnectionContext(
        address=host,
        port=443,
        user=str(os.getenv("HANA_USER")),
        password=str(os.getenv("HANA_PASSWORD")),
    )
    return connection


# getting connection object
conn = get_hana_db()

def ensure_embeddings_table():
    """
    create DOCUMENTS_EMBEDDING table
    """
    conn = get_hana_db()
    try:
        exists_sql = """
        SELECT 1 FROM SYS.TABLES
        WHERE TABLE_NAME = 'DOCUMENTS_EMBEDDING'
        AND SCHEMA_NAME = CURRENT_SCHEMA
        """
        result = conn.sql(exists_sql).collect()
        if not result.empty:
            logger.info("DOCUMENTS_EMBEDDING table already exists...")
            return

        create_table_sql = """
CREATE COLUMN TABLE DOCUMENTS_EMBEDDING (
    id INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    document_text NVARCHAR(5000),
    embedding REAL_VECTOR(3072),
    chunk_metadata NVARCHAR(1000),
    ref_id NVARCHAR(36) UNIQUE NOT NULL
)
"""     
        conn.execute_sql(create_table_sql)
        logger.info("DOCUMENTS_EMBEDDING table created successfully...")
        
    except Exception as e:
        logger.error(f"Error ensuring embeddings table: {e}")

def ensure_triple_store():
    """
    create TRIPLE STORE table and indexes if they don't exist
    """
    conn = get_hana_db()
    try:
        exists_sql = """
        SELECT 1 FROM SYS.TABLES
        WHERE TABLE_NAME = 'TRIPLE_STORE'
        AND SCHEMA_NAME = CURRENT_SCHEMA
        """
        
        result = conn.sql(exists_sql).collect()
        if not result.empty:
            logger.info("triple store already exists...")
            return
        
        create_sql = """
        CREATE COLUMN TABLE TRIPLE_STORE (
        ID BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
        EMB_REF_ID NVARCHAR(36) NOT NULL,
        CHUNK_INDEX INTEGER,
        SUBJECT NVARCHAR(500),
        PREDICATE NVARCHAR(200),
        OBJECT NVARCHAR(1000),
        CREATED_AT TIMESTAMP DEFAULT CURRENT_UTCTIMESTAMP,
        FOREIGN KEY (EMB_REF_ID) REFERENCES DOCUMENTS_EMBEDDING(ref_id)
        )
        """
        conn.execute_sql(create_sql)
        conn.execute_sql("CREATE INDEX IDX_TRIPLE_SUBJ ON TRIPLE_STORE (SUBJECT)")
        conn.execute_sql("CREATE INDEX IDX_TRIPLE_PRED ON TRIPLE_STORE (PREDICATE)")
        conn.execute_sql("CREATE INDEX IDX_TRIPLE_OBJ ON TRIPLE_STORE (OBJECT)")
        conn.execute_sql(
            "CREATE INDEX IDX_TRIPLE_REF ON TRIPLE_STORE (EMB_REF_ID, CHUNK_INDEX)"
        )
        logger.info("Successfully created triple store and indexes...")

    except Exception as e:
        print(f"error ensuring triple store: {e}")


def insert_triplets(triplets_rows: list):
    """
    Insert triplets with their metadata
    triplets_rows: list of (ref_id, chunk_index, subject, predicate, object) tuples
    """
    ensure_triple_store()
    conn = get_hana_db()

    if not triplets_rows:
        logger.warning("No triplets to insert")
        return False
    
    # triplets_rows already has the right format: (ref_id, chunk_index, subj, pred, obj)
    sql = """
        INSERT INTO TRIPLE_STORE (EMB_REF_ID, CHUNK_INDEX, SUBJECT, PREDICATE, OBJECT)
        VALUES (?, ?, ?, ?, ?)
    """
    try:
        cur = conn.connection.cursor()
        cur.executemany(sql, triplets_rows)
        conn.connection.commit()
        cur.close()
        logger.info(f"Inserted {len(triplets_rows)} triplets")
        return True
    except Exception as e:
        logger.error(f"Error inserting triplets: {e}")
        
        return False
    
# this function will insert embeddings and will return the document ID's
def batch_insertion_embedding(rows):
    ensure_embeddings_table()
    conn_ctx = get_hana_db()
    
    """
    args:
        - rows : list of tuples -> [(document_text: str, embedding_string: str, metadata_json: str, ref_id: str), ...]
    """
    sql = "INSERT INTO DOCUMENTS_EMBEDDING (document_text, embedding, chunk_metadata, ref_id) VALUES (?, TO_REAL_VECTOR(?), ?, ?)"
    logger.info(f"Batch inserting embeddings: {rows}")

    if not rows:
        logger.warning("No rows to insert")
        return False
    
    try:
        cursor = conn_ctx.connection.cursor()
        cursor.executemany(sql, rows)
        conn_ctx.connection.commit()
        cursor.close()
        return True
    except Exception as e:
        logger.error(f"Failed to execute batch insert: {e}")
        return False


def insert_embedding(document_text, embedding_vector, chunk_metadata=None):
    conn = get_hana_db()

    # escaping text for SQL
    escaped_text = document_text.replace("'", "''")

    # Converting metadata dict to JSON string and escaping it
    if chunk_metadata:
        metadata_json = json.dumps(chunk_metadata)
        escaped_metadata = metadata_json.replace("'", "''")
    else:
        escaped_metadata = "{}"

    insert_sql = f"""
    INSERT INTO DOCUMENTS_EMBEDDING (document_text, embedding, chunk_metadata)
    VALUES ('{escaped_text}', TO_REAL_VECTOR('{str(embedding_vector)}'), '{escaped_metadata}')
    """

    try:
        conn.execute_sql(insert_sql)
        logger.info("Successfully inserted document and embedding")
        return True
    except Exception as e:
        logger.error(f"Error inserting document and embedding: {e}")
        return False


# function to find top k similiar documents
# ...existing code...
async def search_similiar_documents(query_embedding, top_k=5):
    conn = get_hana_db()

    # Escape and format inputs
    vec = str(query_embedding).replace("'", "''")
    k = int(top_k) if top_k else 5
    if k <= 0:
        k = 5

    search_sql = f"""
    SELECT DOCUMENT_TEXT,
           CHUNK_METADATA,
           COSINE_SIMILARITY(EMBEDDING, TO_REAL_VECTOR('{vec}')) AS SIMILARITY
    FROM DOCUMENTS_EMBEDDING
    ORDER BY SIMILARITY DESC
    LIMIT {k}
    """
    try:
        df = conn.sql(search_sql)
        results = df.collect()
        print(results)
        return results
    except Exception as e:
        print(f"error searching similar documents: {e}")
        return None


# function to get all the data from database
def get_all_data():
    conn = get_hana_db()
    sql = "SELECT * FROM DOCUMENTS_EMBEDDING"
    try:
        df = conn.sql(sql)
        results = df.collect()
        print(results)
        return results
    except Exception as e:
        print(f"error fetching all data: {e}")
        return None
